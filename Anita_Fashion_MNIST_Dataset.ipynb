{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DCTYWbsF_KQo"
      },
      "outputs": [],
      "source": [
        "import torch #Menyediakan fungsi dasar untuk operasi tensor dan komputasi berbasis GPU.\n",
        "import torch.nn as nn #Menyediakan modul dan kelas untuk membangun jaringan saraf.\n",
        "import torch.optim as optim #Menyediakan algoritma pengoptimalan untuk melatih model.\n",
        "from torch.utils.data import DataLoader, random_split #Menyediakan kelas dan fungsi untuk memuat dan memproses data.\n",
        "import torchvision #Menyediakan kumpulan data, model, dan transformasi gambar yang telah dibuat sebelumnya.\n",
        "import torchvision.transforms as transforms #Menyediakan transformasi gambar umum untuk augmentasi data.\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau #Menyediakan scheduler laju pembelajaran untuk menyesuaikan laju pembelajaran selama pelatihan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TPRFUvb5_YsQ"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "#Transformasi Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOhIYXQY_bIX",
        "outputId": "8255c759-2db9-4c3a-c55f-316dd55442f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 208kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.88MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 4.04MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "#Load Fashion MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B6o4GmLB_fcI"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "#Split train dataset menjadi train dan validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JJI1hZFS_hin"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, kernel_size=3, pooling_type='max'):\n",
        "        super(CNN, self).__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=kernel_size, padding=padding)  # Input 1 channel (grayscale)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=padding)\n",
        "\n",
        "        if pooling_type == 'max':\n",
        "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        elif pooling_type == 'avg':\n",
        "            self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Size of feature map after pooling: 7x7\n",
        "        self.fc2 = nn.Linear(128, 10)  # 10 classes in Fashion MNIST\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "#Definisi Arsitektur CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wO6gct4T_ii4",
        "outputId": "de9fe0e8-017f-4d10-d3c0-205e2512952d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.6210, Val Loss: 0.3640\n",
            "Epoch 2/5, Loss: 0.3856, Val Loss: 0.3066\n",
            "Epoch 3/5, Loss: 0.3249, Val Loss: 0.2868\n",
            "Epoch 4/5, Loss: 0.2872, Val Loss: 0.2631\n",
            "Epoch 5/5, Loss: 0.2606, Val Loss: 0.2471\n",
            "Accuracy: 90.77%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/50, Loss: 0.6148, Val Loss: 0.3564\n",
            "Epoch 2/50, Loss: 0.3821, Val Loss: 0.3197\n",
            "Epoch 3/50, Loss: 0.3165, Val Loss: 0.2695\n",
            "Epoch 4/50, Loss: 0.2873, Val Loss: 0.2524\n",
            "Epoch 5/50, Loss: 0.2548, Val Loss: 0.2481\n",
            "Epoch 6/50, Loss: 0.2371, Val Loss: 0.2381\n",
            "Epoch 7/50, Loss: 0.2217, Val Loss: 0.2336\n",
            "Epoch 8/50, Loss: 0.2083, Val Loss: 0.2368\n",
            "Epoch 9/50, Loss: 0.1973, Val Loss: 0.2358\n",
            "Epoch 10/50, Loss: 0.1844, Val Loss: 0.2391\n",
            "Epoch 11/50, Loss: 0.1739, Val Loss: 0.2404\n",
            "Epoch 12/50, Loss: 0.1690, Val Loss: 0.2440\n",
            "Epoch 13/50, Loss: 0.1615, Val Loss: 0.2381\n",
            "Epoch 14/50, Loss: 0.1198, Val Loss: 0.2339\n",
            "Epoch 15/50, Loss: 0.1122, Val Loss: 0.2411\n",
            "Epoch 16/50, Loss: 0.1038, Val Loss: 0.2464\n",
            "Epoch 17/50, Loss: 0.0968, Val Loss: 0.2576\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.28%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=100\n",
            "Evaluating with params: {'epochs': 100, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/100, Loss: 0.6398, Val Loss: 0.3877\n",
            "Epoch 2/100, Loss: 0.3912, Val Loss: 0.3035\n",
            "Epoch 3/100, Loss: 0.3282, Val Loss: 0.2830\n",
            "Epoch 4/100, Loss: 0.2895, Val Loss: 0.2664\n",
            "Epoch 5/100, Loss: 0.2665, Val Loss: 0.2474\n",
            "Epoch 6/100, Loss: 0.2431, Val Loss: 0.2394\n",
            "Epoch 7/100, Loss: 0.2268, Val Loss: 0.2398\n",
            "Epoch 8/100, Loss: 0.2122, Val Loss: 0.2357\n",
            "Epoch 9/100, Loss: 0.1997, Val Loss: 0.2405\n",
            "Epoch 10/100, Loss: 0.1888, Val Loss: 0.2292\n",
            "Epoch 11/100, Loss: 0.1792, Val Loss: 0.2349\n",
            "Epoch 12/100, Loss: 0.1690, Val Loss: 0.2371\n",
            "Epoch 13/100, Loss: 0.1622, Val Loss: 0.2353\n",
            "Epoch 14/100, Loss: 0.1486, Val Loss: 0.2328\n",
            "Epoch 15/100, Loss: 0.1426, Val Loss: 0.2387\n",
            "Epoch 16/100, Loss: 0.1381, Val Loss: 0.2544\n",
            "Epoch 17/100, Loss: 0.1060, Val Loss: 0.2591\n",
            "Epoch 18/100, Loss: 0.0942, Val Loss: 0.2648\n",
            "Epoch 19/100, Loss: 0.0849, Val Loss: 0.2673\n",
            "Epoch 20/100, Loss: 0.0832, Val Loss: 0.2778\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.48%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=250\n",
            "Evaluating with params: {'epochs': 250, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/250, Loss: 0.6169, Val Loss: 0.3648\n",
            "Epoch 2/250, Loss: 0.3816, Val Loss: 0.3224\n",
            "Epoch 3/250, Loss: 0.3222, Val Loss: 0.2782\n",
            "Epoch 4/250, Loss: 0.2900, Val Loss: 0.2754\n",
            "Epoch 5/250, Loss: 0.2651, Val Loss: 0.2456\n",
            "Epoch 6/250, Loss: 0.2414, Val Loss: 0.2477\n",
            "Epoch 7/250, Loss: 0.2298, Val Loss: 0.2365\n",
            "Epoch 8/250, Loss: 0.2148, Val Loss: 0.2319\n",
            "Epoch 9/250, Loss: 0.1996, Val Loss: 0.2541\n",
            "Epoch 10/250, Loss: 0.1925, Val Loss: 0.2399\n",
            "Epoch 11/250, Loss: 0.1781, Val Loss: 0.2288\n",
            "Epoch 12/250, Loss: 0.1679, Val Loss: 0.2389\n",
            "Epoch 13/250, Loss: 0.1610, Val Loss: 0.2470\n",
            "Epoch 14/250, Loss: 0.1533, Val Loss: 0.2368\n",
            "Epoch 15/250, Loss: 0.1489, Val Loss: 0.2566\n",
            "Epoch 16/250, Loss: 0.1407, Val Loss: 0.2409\n",
            "Epoch 17/250, Loss: 0.1328, Val Loss: 0.2697\n",
            "Epoch 18/250, Loss: 0.0985, Val Loss: 0.2602\n",
            "Epoch 19/250, Loss: 0.0869, Val Loss: 0.2682\n",
            "Epoch 20/250, Loss: 0.0804, Val Loss: 0.2958\n",
            "Epoch 21/250, Loss: 0.0757, Val Loss: 0.2737\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.04%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=SGD, epochs=350\n",
            "Evaluating with params: {'epochs': 350, 'kernel_size': 3, 'optimizer_type': 'SGD', 'pooling_type': 'max'}\n",
            "Epoch 1/350, Loss: 0.6160, Val Loss: 0.3653\n",
            "Epoch 2/350, Loss: 0.3757, Val Loss: 0.3045\n",
            "Epoch 3/350, Loss: 0.3174, Val Loss: 0.2874\n",
            "Epoch 4/350, Loss: 0.2791, Val Loss: 0.2571\n",
            "Epoch 5/350, Loss: 0.2585, Val Loss: 0.2562\n",
            "Epoch 6/350, Loss: 0.2349, Val Loss: 0.2415\n",
            "Epoch 7/350, Loss: 0.2230, Val Loss: 0.2399\n",
            "Epoch 8/350, Loss: 0.2100, Val Loss: 0.2411\n",
            "Epoch 9/350, Loss: 0.1938, Val Loss: 0.2403\n",
            "Epoch 10/350, Loss: 0.1834, Val Loss: 0.2458\n",
            "Epoch 11/350, Loss: 0.1735, Val Loss: 0.2326\n",
            "Epoch 12/350, Loss: 0.1646, Val Loss: 0.2446\n",
            "Epoch 13/350, Loss: 0.1580, Val Loss: 0.2472\n",
            "Epoch 14/350, Loss: 0.1502, Val Loss: 0.2564\n",
            "Epoch 15/350, Loss: 0.1390, Val Loss: 0.2513\n",
            "Epoch 16/350, Loss: 0.1332, Val Loss: 0.2566\n",
            "Epoch 17/350, Loss: 0.1251, Val Loss: 0.2743\n",
            "Epoch 18/350, Loss: 0.0931, Val Loss: 0.2644\n",
            "Epoch 19/350, Loss: 0.0838, Val Loss: 0.2824\n",
            "Epoch 20/350, Loss: 0.0762, Val Loss: 0.2858\n",
            "Epoch 21/350, Loss: 0.0733, Val Loss: 0.2848\n",
            "Early stopping triggered.\n",
            "Accuracy: 92.18%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=5\n",
            "Evaluating with params: {'epochs': 5, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/5, Loss: 2.6414, Val Loss: 0.5216\n",
            "Epoch 2/5, Loss: 0.6285, Val Loss: 0.4618\n",
            "Epoch 3/5, Loss: 0.5866, Val Loss: 0.4410\n",
            "Epoch 4/5, Loss: 0.5358, Val Loss: 0.4665\n",
            "Epoch 5/5, Loss: 0.5236, Val Loss: 0.4179\n",
            "Accuracy: 84.62%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=50\n",
            "Evaluating with params: {'epochs': 50, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/50, Loss: 1.8006, Val Loss: 0.5489\n",
            "Epoch 2/50, Loss: 0.5880, Val Loss: 0.4704\n",
            "Epoch 3/50, Loss: 0.5486, Val Loss: 0.4247\n",
            "Epoch 4/50, Loss: 0.6519, Val Loss: 0.4818\n",
            "Epoch 5/50, Loss: 0.5264, Val Loss: 0.4832\n",
            "Epoch 6/50, Loss: 0.5228, Val Loss: 0.4604\n",
            "Epoch 7/50, Loss: 0.5304, Val Loss: 0.4249\n",
            "Epoch 8/50, Loss: 0.5146, Val Loss: 0.4104\n",
            "Epoch 9/50, Loss: 0.5700, Val Loss: 0.4632\n",
            "Epoch 10/50, Loss: 0.5572, Val Loss: 0.4458\n",
            "Epoch 11/50, Loss: 0.5489, Val Loss: 0.4724\n",
            "Epoch 12/50, Loss: 0.5546, Val Loss: 0.4824\n",
            "Epoch 13/50, Loss: 0.5482, Val Loss: 0.5696\n",
            "Epoch 14/50, Loss: 0.5521, Val Loss: 0.4675\n",
            "Epoch 15/50, Loss: 0.4732, Val Loss: 0.4125\n",
            "Epoch 16/50, Loss: 0.4550, Val Loss: 0.4078\n",
            "Epoch 17/50, Loss: 0.4498, Val Loss: 0.4017\n",
            "Epoch 18/50, Loss: 0.4524, Val Loss: 0.4093\n",
            "Epoch 19/50, Loss: 0.4530, Val Loss: 0.4172\n",
            "Epoch 20/50, Loss: 0.4483, Val Loss: 0.4280\n",
            "Epoch 21/50, Loss: 0.4520, Val Loss: 0.4234\n",
            "Epoch 22/50, Loss: 0.4503, Val Loss: 0.6206\n",
            "Epoch 23/50, Loss: 0.4527, Val Loss: 0.4127\n",
            "Epoch 24/50, Loss: 0.4102, Val Loss: 0.3714\n",
            "Epoch 25/50, Loss: 0.3974, Val Loss: 0.3775\n",
            "Epoch 26/50, Loss: 0.3992, Val Loss: 0.3736\n",
            "Epoch 27/50, Loss: 0.3974, Val Loss: 0.3735\n",
            "Epoch 28/50, Loss: 0.3937, Val Loss: 0.3696\n",
            "Epoch 29/50, Loss: 0.4006, Val Loss: 0.3893\n",
            "Epoch 30/50, Loss: 0.3966, Val Loss: 0.3836\n",
            "Epoch 31/50, Loss: 0.3939, Val Loss: 0.3960\n",
            "Epoch 32/50, Loss: 0.3949, Val Loss: 0.3683\n",
            "Epoch 33/50, Loss: 0.3909, Val Loss: 0.3733\n",
            "Epoch 34/50, Loss: 0.3928, Val Loss: 0.3725\n",
            "Epoch 35/50, Loss: 0.3902, Val Loss: 0.3740\n",
            "Epoch 36/50, Loss: 0.3945, Val Loss: 0.4018\n",
            "Epoch 37/50, Loss: 0.3943, Val Loss: 0.3829\n",
            "Epoch 38/50, Loss: 0.4053, Val Loss: 0.3865\n",
            "Epoch 39/50, Loss: 0.3651, Val Loss: 0.3603\n",
            "Epoch 40/50, Loss: 0.3671, Val Loss: 0.3601\n",
            "Epoch 41/50, Loss: 0.3615, Val Loss: 0.3598\n",
            "Epoch 42/50, Loss: 0.3640, Val Loss: 0.3577\n",
            "Epoch 43/50, Loss: 0.3606, Val Loss: 0.3640\n",
            "Epoch 44/50, Loss: 0.3635, Val Loss: 0.3571\n",
            "Epoch 45/50, Loss: 0.3612, Val Loss: 0.3556\n",
            "Epoch 46/50, Loss: 0.3588, Val Loss: 0.3664\n",
            "Epoch 47/50, Loss: 0.3595, Val Loss: 0.3671\n",
            "Epoch 48/50, Loss: 0.3630, Val Loss: 0.3563\n",
            "Epoch 49/50, Loss: 0.3623, Val Loss: 0.3812\n",
            "Epoch 50/50, Loss: 0.3584, Val Loss: 0.3564\n",
            "Accuracy: 87.20%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=100\n",
            "Evaluating with params: {'epochs': 100, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/100, Loss: 1.6147, Val Loss: 0.5895\n",
            "Epoch 2/100, Loss: 0.6107, Val Loss: 0.4449\n",
            "Epoch 3/100, Loss: 0.5724, Val Loss: 0.4601\n",
            "Epoch 4/100, Loss: 0.5288, Val Loss: 0.4107\n",
            "Epoch 5/100, Loss: 0.5198, Val Loss: 0.4192\n",
            "Epoch 6/100, Loss: 0.5000, Val Loss: 0.4669\n",
            "Epoch 7/100, Loss: 0.4870, Val Loss: 0.5056\n",
            "Epoch 8/100, Loss: 0.4988, Val Loss: 0.4085\n",
            "Epoch 9/100, Loss: 0.4877, Val Loss: 0.4138\n",
            "Epoch 10/100, Loss: 0.4890, Val Loss: 0.4226\n",
            "Epoch 11/100, Loss: 0.4890, Val Loss: 0.4305\n",
            "Epoch 12/100, Loss: 0.4921, Val Loss: 0.4302\n",
            "Epoch 13/100, Loss: 0.4954, Val Loss: 0.4443\n",
            "Epoch 14/100, Loss: 0.4947, Val Loss: 0.4279\n",
            "Epoch 15/100, Loss: 0.4332, Val Loss: 0.4020\n",
            "Epoch 16/100, Loss: 0.4205, Val Loss: 0.4016\n",
            "Epoch 17/100, Loss: 0.4210, Val Loss: 0.4034\n",
            "Epoch 18/100, Loss: 0.4222, Val Loss: 0.3836\n",
            "Epoch 19/100, Loss: 0.4195, Val Loss: 0.3864\n",
            "Epoch 20/100, Loss: 0.4205, Val Loss: 0.3974\n",
            "Epoch 21/100, Loss: 0.4195, Val Loss: 0.3924\n",
            "Epoch 22/100, Loss: 0.4137, Val Loss: 0.3939\n",
            "Epoch 23/100, Loss: 0.4178, Val Loss: 0.3867\n",
            "Epoch 24/100, Loss: 0.4136, Val Loss: 0.3925\n",
            "Epoch 25/100, Loss: 0.3807, Val Loss: 0.3754\n",
            "Epoch 26/100, Loss: 0.3772, Val Loss: 0.3688\n",
            "Epoch 27/100, Loss: 0.3754, Val Loss: 0.3866\n",
            "Epoch 28/100, Loss: 0.3734, Val Loss: 0.3689\n",
            "Epoch 29/100, Loss: 0.3725, Val Loss: 0.3747\n",
            "Epoch 30/100, Loss: 0.3737, Val Loss: 0.3739\n",
            "Epoch 31/100, Loss: 0.3711, Val Loss: 0.3561\n",
            "Epoch 32/100, Loss: 0.3675, Val Loss: 0.3718\n",
            "Epoch 33/100, Loss: 0.3719, Val Loss: 0.3561\n",
            "Epoch 34/100, Loss: 0.3696, Val Loss: 0.3567\n",
            "Epoch 35/100, Loss: 0.3720, Val Loss: 0.3690\n",
            "Epoch 36/100, Loss: 0.3756, Val Loss: 0.3535\n",
            "Epoch 37/100, Loss: 0.3693, Val Loss: 0.3597\n",
            "Epoch 38/100, Loss: 0.3701, Val Loss: 0.3892\n",
            "Epoch 39/100, Loss: 0.3690, Val Loss: 0.3567\n",
            "Epoch 40/100, Loss: 0.3692, Val Loss: 0.3744\n",
            "Epoch 41/100, Loss: 0.3734, Val Loss: 0.3565\n",
            "Epoch 42/100, Loss: 0.3673, Val Loss: 0.3786\n",
            "Epoch 43/100, Loss: 0.3466, Val Loss: 0.3626\n",
            "Epoch 44/100, Loss: 0.3464, Val Loss: 0.3496\n",
            "Epoch 45/100, Loss: 0.3436, Val Loss: 0.3501\n",
            "Epoch 46/100, Loss: 0.3392, Val Loss: 0.3660\n",
            "Epoch 47/100, Loss: 0.3416, Val Loss: 0.3661\n",
            "Epoch 48/100, Loss: 0.3427, Val Loss: 0.3590\n",
            "Epoch 49/100, Loss: 0.3432, Val Loss: 0.3579\n",
            "Epoch 50/100, Loss: 0.3443, Val Loss: 0.3495\n",
            "Epoch 51/100, Loss: 0.3433, Val Loss: 0.3496\n",
            "Epoch 52/100, Loss: 0.3402, Val Loss: 0.3605\n",
            "Epoch 53/100, Loss: 0.3419, Val Loss: 0.3478\n",
            "Epoch 54/100, Loss: 0.3442, Val Loss: 0.3780\n",
            "Epoch 55/100, Loss: 0.3407, Val Loss: 0.3476\n",
            "Epoch 56/100, Loss: 0.3422, Val Loss: 0.3470\n",
            "Epoch 57/100, Loss: 0.3392, Val Loss: 0.3523\n",
            "Epoch 58/100, Loss: 0.3395, Val Loss: 0.3446\n",
            "Epoch 59/100, Loss: 0.3449, Val Loss: 0.3675\n",
            "Epoch 60/100, Loss: 0.3419, Val Loss: 0.3519\n",
            "Epoch 61/100, Loss: 0.3420, Val Loss: 0.3460\n",
            "Epoch 62/100, Loss: 0.3432, Val Loss: 0.3493\n",
            "Epoch 63/100, Loss: 0.3428, Val Loss: 0.3495\n",
            "Epoch 64/100, Loss: 0.3430, Val Loss: 0.3586\n",
            "Epoch 65/100, Loss: 0.3256, Val Loss: 0.3453\n",
            "Epoch 66/100, Loss: 0.3277, Val Loss: 0.3685\n",
            "Epoch 67/100, Loss: 0.3281, Val Loss: 0.3559\n",
            "Epoch 68/100, Loss: 0.3260, Val Loss: 0.3352\n",
            "Epoch 69/100, Loss: 0.3286, Val Loss: 0.3396\n",
            "Epoch 70/100, Loss: 0.3261, Val Loss: 0.3414\n",
            "Epoch 71/100, Loss: 0.3262, Val Loss: 0.3415\n",
            "Epoch 72/100, Loss: 0.3296, Val Loss: 0.3540\n",
            "Epoch 73/100, Loss: 0.3250, Val Loss: 0.3384\n",
            "Epoch 74/100, Loss: 0.3294, Val Loss: 0.3407\n",
            "Epoch 75/100, Loss: 0.3174, Val Loss: 0.3415\n",
            "Epoch 76/100, Loss: 0.3196, Val Loss: 0.3407\n",
            "Epoch 77/100, Loss: 0.3181, Val Loss: 0.3337\n",
            "Epoch 78/100, Loss: 0.3157, Val Loss: 0.3418\n",
            "Epoch 79/100, Loss: 0.3170, Val Loss: 0.3473\n",
            "Epoch 80/100, Loss: 0.3128, Val Loss: 0.3566\n",
            "Epoch 81/100, Loss: 0.3177, Val Loss: 0.3395\n",
            "Epoch 82/100, Loss: 0.3156, Val Loss: 0.3432\n",
            "Epoch 83/100, Loss: 0.3183, Val Loss: 0.3436\n",
            "Epoch 84/100, Loss: 0.3144, Val Loss: 0.3336\n",
            "Epoch 85/100, Loss: 0.3149, Val Loss: 0.3359\n",
            "Epoch 86/100, Loss: 0.3138, Val Loss: 0.3356\n",
            "Epoch 87/100, Loss: 0.3121, Val Loss: 0.3349\n",
            "Epoch 88/100, Loss: 0.3171, Val Loss: 0.3354\n",
            "Epoch 89/100, Loss: 0.3123, Val Loss: 0.3304\n",
            "Epoch 90/100, Loss: 0.3131, Val Loss: 0.3341\n",
            "Epoch 91/100, Loss: 0.3151, Val Loss: 0.3369\n",
            "Epoch 92/100, Loss: 0.3112, Val Loss: 0.3332\n",
            "Epoch 93/100, Loss: 0.3144, Val Loss: 0.3352\n",
            "Epoch 94/100, Loss: 0.3126, Val Loss: 0.3339\n",
            "Epoch 95/100, Loss: 0.3132, Val Loss: 0.3348\n",
            "Epoch 96/100, Loss: 0.3101, Val Loss: 0.3365\n",
            "Epoch 97/100, Loss: 0.3106, Val Loss: 0.3363\n",
            "Epoch 98/100, Loss: 0.3116, Val Loss: 0.3363\n",
            "Epoch 99/100, Loss: 0.3091, Val Loss: 0.3395\n",
            "Early stopping triggered.\n",
            "Accuracy: 88.02%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=250\n",
            "Evaluating with params: {'epochs': 250, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/250, Loss: 2.4198, Val Loss: 0.5461\n",
            "Epoch 2/250, Loss: 0.6826, Val Loss: 0.5411\n",
            "Epoch 3/250, Loss: 0.6205, Val Loss: 0.5022\n",
            "Epoch 4/250, Loss: 0.5535, Val Loss: 0.4525\n",
            "Epoch 5/250, Loss: 0.5330, Val Loss: 0.4496\n",
            "Epoch 6/250, Loss: 0.5303, Val Loss: 0.4373\n",
            "Epoch 7/250, Loss: 0.5224, Val Loss: 0.4586\n",
            "Epoch 8/250, Loss: 0.5111, Val Loss: 0.4478\n",
            "Epoch 9/250, Loss: 0.5190, Val Loss: 0.4183\n",
            "Epoch 10/250, Loss: 0.5119, Val Loss: 0.4350\n",
            "Epoch 11/250, Loss: 0.5137, Val Loss: 0.4485\n",
            "Epoch 12/250, Loss: 0.5160, Val Loss: 0.4307\n",
            "Epoch 13/250, Loss: 0.5206, Val Loss: 0.4334\n",
            "Epoch 14/250, Loss: 0.5275, Val Loss: 0.4341\n",
            "Epoch 15/250, Loss: 0.5089, Val Loss: 0.4863\n",
            "Epoch 16/250, Loss: 0.4509, Val Loss: 0.3917\n",
            "Epoch 17/250, Loss: 0.4440, Val Loss: 0.3968\n",
            "Epoch 18/250, Loss: 0.4415, Val Loss: 0.3787\n",
            "Epoch 19/250, Loss: 0.4394, Val Loss: 0.3855\n",
            "Epoch 20/250, Loss: 0.4428, Val Loss: 0.3945\n",
            "Epoch 21/250, Loss: 0.4371, Val Loss: 0.4029\n",
            "Epoch 22/250, Loss: 0.4370, Val Loss: 0.3889\n",
            "Epoch 23/250, Loss: 0.4371, Val Loss: 0.3959\n",
            "Epoch 24/250, Loss: 0.4407, Val Loss: 0.3935\n",
            "Epoch 25/250, Loss: 0.4014, Val Loss: 0.3609\n",
            "Epoch 26/250, Loss: 0.3951, Val Loss: 0.3650\n",
            "Epoch 27/250, Loss: 0.3955, Val Loss: 0.3548\n",
            "Epoch 28/250, Loss: 0.3905, Val Loss: 0.3566\n",
            "Epoch 29/250, Loss: 0.3924, Val Loss: 0.3585\n",
            "Epoch 30/250, Loss: 0.3914, Val Loss: 0.3558\n",
            "Epoch 31/250, Loss: 0.3872, Val Loss: 0.3425\n",
            "Epoch 32/250, Loss: 0.3917, Val Loss: 0.3527\n",
            "Epoch 33/250, Loss: 0.3890, Val Loss: 0.3556\n",
            "Epoch 34/250, Loss: 0.3882, Val Loss: 0.3489\n",
            "Epoch 35/250, Loss: 0.3888, Val Loss: 0.3850\n",
            "Epoch 36/250, Loss: 0.3851, Val Loss: 0.3490\n",
            "Epoch 37/250, Loss: 0.3861, Val Loss: 0.3680\n",
            "Epoch 38/250, Loss: 0.3652, Val Loss: 0.3459\n",
            "Epoch 39/250, Loss: 0.3624, Val Loss: 0.3340\n",
            "Epoch 40/250, Loss: 0.3597, Val Loss: 0.3362\n",
            "Epoch 41/250, Loss: 0.3590, Val Loss: 0.3378\n",
            "Epoch 42/250, Loss: 0.3578, Val Loss: 0.3450\n",
            "Epoch 43/250, Loss: 0.3584, Val Loss: 0.3378\n",
            "Epoch 44/250, Loss: 0.3575, Val Loss: 0.3367\n",
            "Epoch 45/250, Loss: 0.3590, Val Loss: 0.3406\n",
            "Epoch 46/250, Loss: 0.3420, Val Loss: 0.3365\n",
            "Epoch 47/250, Loss: 0.3445, Val Loss: 0.3243\n",
            "Epoch 48/250, Loss: 0.3455, Val Loss: 0.3226\n",
            "Epoch 49/250, Loss: 0.3412, Val Loss: 0.3287\n",
            "Epoch 50/250, Loss: 0.3404, Val Loss: 0.3207\n",
            "Epoch 51/250, Loss: 0.3447, Val Loss: 0.3317\n",
            "Epoch 52/250, Loss: 0.3414, Val Loss: 0.3220\n",
            "Epoch 53/250, Loss: 0.3417, Val Loss: 0.3343\n",
            "Epoch 54/250, Loss: 0.3412, Val Loss: 0.3278\n",
            "Epoch 55/250, Loss: 0.3394, Val Loss: 0.3361\n",
            "Epoch 56/250, Loss: 0.3419, Val Loss: 0.3287\n",
            "Epoch 57/250, Loss: 0.3328, Val Loss: 0.3213\n",
            "Epoch 58/250, Loss: 0.3328, Val Loss: 0.3213\n",
            "Epoch 59/250, Loss: 0.3307, Val Loss: 0.3232\n",
            "Epoch 60/250, Loss: 0.3328, Val Loss: 0.3268\n",
            "Early stopping triggered.\n",
            "Accuracy: 88.38%\n",
            "Running experiment with kernel_size=3, pooling_type=max, optimizer=RMSProp, epochs=350\n",
            "Evaluating with params: {'epochs': 350, 'kernel_size': 3, 'optimizer_type': 'RMSProp', 'pooling_type': 'max'}\n",
            "Epoch 1/350, Loss: 3.4198, Val Loss: 0.5515\n",
            "Epoch 2/350, Loss: 0.6703, Val Loss: 0.5806\n",
            "Epoch 3/350, Loss: 0.6170, Val Loss: 0.5125\n",
            "Epoch 4/350, Loss: 0.5753, Val Loss: 0.4846\n",
            "Epoch 5/350, Loss: 0.5520, Val Loss: 0.5586\n",
            "Epoch 6/350, Loss: 0.5433, Val Loss: 0.4814\n",
            "Epoch 7/350, Loss: 0.5386, Val Loss: 0.4411\n",
            "Epoch 8/350, Loss: 0.5222, Val Loss: 0.4190\n",
            "Epoch 9/350, Loss: 0.5176, Val Loss: 0.4100\n",
            "Epoch 10/350, Loss: 0.5244, Val Loss: 0.4759\n",
            "Epoch 11/350, Loss: 0.5348, Val Loss: 0.4125\n",
            "Epoch 12/350, Loss: 0.5307, Val Loss: 0.4402\n",
            "Epoch 13/350, Loss: 0.5405, Val Loss: 0.4795\n",
            "Epoch 14/350, Loss: 0.5359, Val Loss: 0.4618\n",
            "Epoch 15/350, Loss: 0.5464, Val Loss: 0.4372\n",
            "Epoch 16/350, Loss: 0.4540, Val Loss: 0.3930\n",
            "Epoch 17/350, Loss: 0.4479, Val Loss: 0.3768\n",
            "Epoch 18/350, Loss: 0.4505, Val Loss: 0.4068\n",
            "Epoch 19/350, Loss: 0.4383, Val Loss: 0.3649\n",
            "Epoch 20/350, Loss: 0.4412, Val Loss: 0.3651\n",
            "Epoch 21/350, Loss: 0.4298, Val Loss: 0.3967\n",
            "Epoch 22/350, Loss: 0.4345, Val Loss: 0.3939\n",
            "Epoch 23/350, Loss: 0.4366, Val Loss: 0.4049\n",
            "Epoch 24/350, Loss: 0.4343, Val Loss: 0.3812\n",
            "Epoch 25/350, Loss: 0.4278, Val Loss: 0.3804\n",
            "Epoch 26/350, Loss: 0.3906, Val Loss: 0.3607\n",
            "Epoch 27/350, Loss: 0.3882, Val Loss: 0.3519\n",
            "Epoch 28/350, Loss: 0.3883, Val Loss: 0.4346\n",
            "Epoch 29/350, Loss: 0.3845, Val Loss: 0.3741\n",
            "Epoch 30/350, Loss: 0.3838, Val Loss: 0.3582\n",
            "Epoch 31/350, Loss: 0.3780, Val Loss: 0.3400\n",
            "Epoch 32/350, Loss: 0.3840, Val Loss: 0.3537\n",
            "Epoch 33/350, Loss: 0.3856, Val Loss: 0.3416\n",
            "Epoch 34/350, Loss: 0.3824, Val Loss: 0.3622\n",
            "Epoch 35/350, Loss: 0.3800, Val Loss: 0.3414\n",
            "Epoch 36/350, Loss: 0.3791, Val Loss: 0.3570\n",
            "Epoch 37/350, Loss: 0.3775, Val Loss: 0.3575\n",
            "Epoch 38/350, Loss: 0.3588, Val Loss: 0.3342\n",
            "Epoch 39/350, Loss: 0.3489, Val Loss: 0.3483\n",
            "Epoch 40/350, Loss: 0.3478, Val Loss: 0.3442\n",
            "Epoch 41/350, Loss: 0.3523, Val Loss: 0.3317\n",
            "Epoch 42/350, Loss: 0.3490, Val Loss: 0.3216\n",
            "Epoch 43/350, Loss: 0.3467, Val Loss: 0.3261\n",
            "Epoch 44/350, Loss: 0.3496, Val Loss: 0.3577\n",
            "Epoch 45/350, Loss: 0.3470, Val Loss: 0.3337\n",
            "Epoch 46/350, Loss: 0.3473, Val Loss: 0.3481\n",
            "Epoch 47/350, Loss: 0.3462, Val Loss: 0.3444\n",
            "Epoch 48/350, Loss: 0.3490, Val Loss: 0.3369\n",
            "Epoch 49/350, Loss: 0.3341, Val Loss: 0.3191\n",
            "Epoch 50/350, Loss: 0.3324, Val Loss: 0.3148\n",
            "Epoch 51/350, Loss: 0.3335, Val Loss: 0.3152\n",
            "Epoch 52/350, Loss: 0.3325, Val Loss: 0.3176\n",
            "Epoch 53/350, Loss: 0.3303, Val Loss: 0.3231\n",
            "Epoch 54/350, Loss: 0.3318, Val Loss: 0.3150\n",
            "Epoch 55/350, Loss: 0.3298, Val Loss: 0.3194\n",
            "Epoch 56/350, Loss: 0.3282, Val Loss: 0.3273\n",
            "Epoch 57/350, Loss: 0.3224, Val Loss: 0.3133\n",
            "Epoch 58/350, Loss: 0.3209, Val Loss: 0.3231\n",
            "Epoch 59/350, Loss: 0.3188, Val Loss: 0.3200\n",
            "Epoch 60/350, Loss: 0.3172, Val Loss: 0.3236\n",
            "Epoch 61/350, Loss: 0.3215, Val Loss: 0.3133\n",
            "Epoch 62/350, Loss: 0.3205, Val Loss: 0.3177\n",
            "Epoch 63/350, Loss: 0.3194, Val Loss: 0.3177\n",
            "Epoch 64/350, Loss: 0.3177, Val Loss: 0.3095\n",
            "Epoch 65/350, Loss: 0.3174, Val Loss: 0.3119\n",
            "Epoch 66/350, Loss: 0.3164, Val Loss: 0.3105\n",
            "Epoch 67/350, Loss: 0.3170, Val Loss: 0.3144\n",
            "Epoch 68/350, Loss: 0.3153, Val Loss: 0.3147\n",
            "Epoch 69/350, Loss: 0.3156, Val Loss: 0.3098\n",
            "Epoch 70/350, Loss: 0.3133, Val Loss: 0.3097\n",
            "Epoch 71/350, Loss: 0.3129, Val Loss: 0.3106\n",
            "Epoch 72/350, Loss: 0.3133, Val Loss: 0.3141\n",
            "Epoch 73/350, Loss: 0.3129, Val Loss: 0.3074\n",
            "Epoch 74/350, Loss: 0.3132, Val Loss: 0.3123\n",
            "Epoch 75/350, Loss: 0.3130, Val Loss: 0.3110\n",
            "Epoch 76/350, Loss: 0.3125, Val Loss: 0.3123\n",
            "Epoch 77/350, Loss: 0.3127, Val Loss: 0.3110\n",
            "Epoch 78/350, Loss: 0.3127, Val Loss: 0.3087\n",
            "Epoch 79/350, Loss: 0.3141, Val Loss: 0.3087\n",
            "Epoch 80/350, Loss: 0.3130, Val Loss: 0.3088\n"
          ]
        }
      ],
      "source": [
        "def train_and_evaluate(kernel_size, pooling_type, optimizer_type, epochs, early_stopping_patience=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = CNN(kernel_size=kernel_size, pooling_type=pooling_type).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if optimizer_type == 'SGD':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "    elif optimizer_type == 'RMSProp':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=0.01)\n",
        "    elif optimizer_type == 'Adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    #Optimizer\n",
        "\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(f\"Evaluating with params: {{'epochs': {epochs}, 'kernel_size': {kernel_size}, 'optimizer_type': '{optimizer_type}', 'pooling_type': '{pooling_type}'}}\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Step\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "    #Learning Rate Scheduler\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "        #Validation Step\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                break\n",
        "        #Early Stopping\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy\n",
        "    #Evaluasi Akurasi pada Test Dataset\n",
        "\n",
        "#Training dan Evaluasi\n",
        "\n",
        "kernel_sizes = [3, 5, 7]\n",
        "pooling_types = ['max', 'avg']\n",
        "optimizers = ['SGD', 'RMSProp', 'Adam']\n",
        "epoch_list = [5, 50, 100, 250, 350]\n",
        "\n",
        "results = []\n",
        "\n",
        "for kernel_size in kernel_sizes:\n",
        "    for pooling_type in pooling_types:\n",
        "        for optimizer in optimizers:\n",
        "            for epochs in epoch_list:\n",
        "                print(f\"Running experiment with kernel_size={kernel_size}, pooling_type={pooling_type}, optimizer={optimizer}, epochs={epochs}\")\n",
        "                accuracy = train_and_evaluate(kernel_size, pooling_type, optimizer, epochs, early_stopping_patience=10)\n",
        "                results.append((kernel_size, pooling_type, optimizer, epochs, accuracy))\n",
        "\n",
        "# Menampilkan Hasil Akhir\n",
        "print(\"\\nHasil Akhir Eksperimen:\")\n",
        "for result in results:\n",
        "    print(f\"Kernel Size: {result[0]}, Pooling: {result[1]}, Optimizer: {result[2]}, Epochs: {result[3]}, Accuracy: {result[4]:.2f}%\")\n",
        "# Menampilkan Hasil Akhir\n",
        "\n",
        "#Eksperimen dengan Semua Kombinasi Parameter"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}